{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn.externals.six'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-9f50fd1431f7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfeature_extraction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexternals\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msix\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStringIO\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpydot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn.externals.six'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import feature_extraction\n",
    "from sklearn import tree\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.core.display import Image\n",
    "import pydot\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "# turn of data table rendering\n",
    "pd.set_option('display.notebook_repr_html', False)\n",
    "sns.set_palette(['#00A99D', '#F5CA0C', '#B6129F', '#76620C', '#095C57'])\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Decision Tree?\n",
    "A decision tree is a structure of questions and answers used to separate data points into classes. We can use supervised machine learning to build such a structure from existing data. Decision trees can be used for classification and regression. In this example we focus on classification.\n",
    "\n",
    "### Classification of Guitar Models\n",
    "In the example below we try to predict the class of a guitar, based on its features, using a decision tree. There are only two classes of guitar models in this case, 10 Stratocasters (st) and 16 Les Paul (lp) models. Our feature set contains body material, fretboard, number of frets and kind of pickup elements. Note: this is probably *highly inaccurate* toy-data only created to illustrate a point. You can [download the data set](https://raw.githubusercontent.com/remondo/NoteBooks-Unsupervised-Learning/master/data/guitar-model.csv) from my GitHub repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the guitar model data set\n",
    "df = pd.read_csv('data\\guitar-model.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Feature Extraction\n",
    "We are confronted with a lot of categorical data, so we need to do some feature extraction first. We use [binary one-hot encoding](http://unsupervised-learning.com/binary-one-hot-encoding-for-machine-learning-in-python/) for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some feature extracting for\n",
    "cat_columns = ['material', 'fretboard', 'frets', 'elements']\n",
    "cat_dict = df[cat_columns].to_dict(orient='records')\n",
    "\n",
    "vec = feature_extraction.DictVectorizer()\n",
    "cat_vector = vec.fit_transform(cat_dict).toarray()\n",
    "\n",
    "df_vector = pd.DataFrame(cat_vector)\n",
    "vector_columns = vec.get_feature_names()\n",
    "df_vector.columns = vector_columns\n",
    "df_vector.index = df.index\n",
    "\n",
    "df = df.drop(cat_columns, axis=1)\n",
    "df = df.join(df_vector)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign an ID to the models\n",
    "df.loc[df.model == 'st','model'] = 0\n",
    "df.loc[df.model == 'lp','model'] = 1\n",
    "df.model.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Decision Tree Classifier\n",
    "We use Scikit Learn's [DecisionTreeClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) to construct a decision tree. To choose which feature gives the largest information gain at any given point in the tree, we use the entropy criterion. Entropy is a proportional measure of how pure a set of labels is, where 0.0 is perfectly pure and 1.0 is the largets possible mix of labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data set in features and labels\n",
    "features = df.drop(['model'], axis=1)\n",
    "labels = df.model\n",
    "\n",
    "test_features = features[-1:]\n",
    "test_label = labels[-1:]\n",
    "\n",
    "# Train the decision tree based on the entropy criterion\n",
    "clf = tree.DecisionTreeClassifier(criterion='entropy')\n",
    "clf = clf.fit(features[:-1], labels[:-1])\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the decision tree\n",
    "dot_data = StringIO() \n",
    "tree.export_graphviz(clf, out_file=dot_data, feature_names=features.columns) \n",
    "graph = pydot.graph_from_dot_data(dot_data.getvalue()) \n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction with test data\n",
    "pred = clf.predict(test_features)\n",
    "print((features[-1:].T))\n",
    "print(('Predicted class:', pred))\n",
    "print(('Accurate prediction?', pred[0] == test_label.values[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the 'material' of the guitar body does not play any roll in deciding which label belongs to a given feature set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Done!\n",
    "\n",
    "#### Next: _Shannon's Entropy and Information Gain for Decision Trees_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
